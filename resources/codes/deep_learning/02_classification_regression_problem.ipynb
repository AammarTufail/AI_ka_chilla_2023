{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><span style=\n",
    "  \"\n",
    "  font-size: 80px; \n",
    "  font-weight: bold;\n",
    "  color: Yellow;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "  \"\n",
    ">\n",
    "   Deep Learning <!--  paste your text -->\n",
    "  </span></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\n",
    "  \"\n",
    "  font-size: 50px; \n",
    "  font-weight: bold;\n",
    "  color: Yellow;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "  \"\n",
    ">\n",
    "   Activation Functions <!--  paste your text -->\n",
    "  </span></center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "| Activation Function | Common Use | Advantages | Disadvantages |\n",
    "|---|---|---|---|\n",
    "| Sigmoid / Logistic | Predicting the probability as output | Outputs between 0 and 1, useful for binary classification, differentiable and provides smooth gradient | Suffers from vanishing gradient problem, output not symmetric around zero |\n",
    "| Tanh (Hyperbolic Tangent) | Hidden layers of neural network | Output is zero-centered, helps centering the data | Suffers from vanishing gradient problem, gradient is steeper compared to sigmoid |\n",
    "| ReLU (Rectified Linear Unit) | Hidden layers of neural network | Computationally efficient, accelerates convergence of gradient descent | Suffers from \"dying ReLU\" problem, all negative input values become zero |\n",
    "| Leaky ReLU | To avoid dying ReLU problem | Enables backpropagation for negative input values, avoids dead neurons | Predictions may not be consistent for negative input values, learning of model parameters is time-consuming |\n",
    "| Parametric ReLU | When Leaky ReLU fails at solving dead neurons problem | Slope of the negative part can be learnt during backpropagation | Performance varies depending on the value of slope parameter 'a' |\n",
    "| Exponential Linear Units (ELUs) | Modifies slope of the negative part of the function | Smoothly approaches the value of -Î± for negative inputs, avoids dead ReLU problem | Increases computational time, no learning of the 'a' value, suffers from exploding gradient problem |\n",
    "| Softmax | Output layer of classifier to represent probability distribution | Output is a probability distribution over 'n' classes | Limitations when dealing with non-exclusive classes |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  text-align: left;\n",
    "  font-size: 25px;\n",
    "  font-weight: bold;\n",
    "  color: Orange;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "\">\n",
    "  Let's Make a Neural Network Using Tensorflow <!-- Paste your text -->\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas seabron matplotlib sklearn plotly openpyxl ipykernel tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 12.6429\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 11.7570\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 10.8971\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 10.0620\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 9.2177\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 8.3366\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 7.4228\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 6.4905\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5.5506\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4.6113\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3.7323\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.9804\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4077\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.0480\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.8413\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.7456\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.6881\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.6372\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.5848\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.5408\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.4934\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.4583\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.4285\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3871\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3628\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.3424\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.3242\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3086\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2939\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2761\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2584\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2539\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2439\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2344\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2202\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2102\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2030\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1964\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1905\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1834\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1803\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1760\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1660\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1625\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1599\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1577\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1550\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1583\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1514\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1477\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7175\n",
      "Test Loss: 0.7174870371818542\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Load the tips dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# label encode the data\n",
    "le = LabelEncoder()\n",
    "tips['sex'] = le.fit_transform(tips['sex'])\n",
    "tips['smoker'] = le.fit_transform(tips['smoker'])\n",
    "tips['day'] = le.fit_transform(tips['day'])\n",
    "tips['time'] = le.fit_transform(tips['time'])\n",
    "\n",
    "# Preprocess the data\n",
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# what are the main types of data scaling and why do we do that?\n",
    "# Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a simple neural network model\n",
    "input_layer = keras.layers.Input(input_shape=(6,)) # input \n",
    "hidden_layer = keras.layers.Dense(16, activation='relu') # hidden layer\n",
    "output_layer = keras.layers.Dense(1) # output layer\n",
    "\n",
    "model = keras.Sequential([input_layer, hidden_layer, output_layer])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if your GPU is workign after installation of tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is just to check out the gpu performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "Num GPUs Available:  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x2997c6d40> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x2997c6d40>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x2997c6d40> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x2997c6d40>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function normalize_img at 0x2997c6d40> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x2997c6d40>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/12\n",
      "469/469 [==============================] - 13s 20ms/step - loss: 0.1588 - accuracy: 0.9524 - val_loss: 0.0463 - val_accuracy: 0.9854\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0438 - accuracy: 0.9866 - val_loss: 0.0366 - val_accuracy: 0.9874\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0382 - val_accuracy: 0.9875\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0372 - val_accuracy: 0.9879\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0346 - val_accuracy: 0.9892\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 0.0322 - val_accuracy: 0.9900\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.0449 - val_accuracy: 0.9877\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.0495 - val_accuracy: 0.9876\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0376 - val_accuracy: 0.9887\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0390 - val_accuracy: 0.9902\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0565 - val_accuracy: 0.9874\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0405 - val_accuracy: 0.9898\n",
      "CPU times: user 59.5 s, sys: 30.2 s, total: 1min 29s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29b487550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=12,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 28ms/step - loss: 0.7047 - accuracy: 0.5077\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6735 - accuracy: 0.5538\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6533 - accuracy: 0.6462\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6339 - accuracy: 0.7128\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6220 - accuracy: 0.7128\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7024 - accuracy: 0.5510\n",
      "Test Loss: 0.7023866772651672\n",
      "Test Accuracy: 0.5510203838348389\n"
     ]
    }
   ],
   "source": [
    "# Step: 1 Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# import the dataset\n",
    "df = sns.load_dataset(\"tips\")\n",
    "\n",
    "# preprocess the data\n",
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['smoker'] = le.fit_transform(df['smoker'])\n",
    "df['day'] = le.fit_transform(df['day'])\n",
    "df['time'] = le.fit_transform(df['time'])\n",
    "\n",
    "# convert 'tip' to a binary variable, 1 if the tip is above $2, else 0\n",
    "df['tip'] = (df['tip'] > 2).astype(int)\n",
    "\n",
    "X = df.drop(\"tip\", axis=1)\n",
    "y = df[\"tip\"]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creata a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=[X_train.shape[1]]), # input layer + Hidden layer\n",
    "    tf.keras.layers.Dense(8, activation='relu'), # hidden layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1) # verbose = [0,1,2], Batch_size=number_of_samples in one iteration\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
