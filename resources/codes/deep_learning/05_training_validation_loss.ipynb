{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training vs Validation loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Link to blog, Must Read](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several typical patterns we can observe in training and validation loss curves during the training process of a machine learning model. Here are some and what they might indicate:\n",
    "\n",
    "1. **`Ideal Case:`** The training loss and the validation loss both decrease with time and then plateau at some point. This is the ideal scenario showing that the model is learning correctly, and it's not overfitting or underfitting. It has generalized well to the validation data.\n",
    "\n",
    "2. **`Overfitting:`** If the training loss continues to decrease with time but the validation loss decreases initially and then starts increasing, this means that the model is overfitting. This happens when the model learns the training data \"too well\", capturing noise and outliers, and fails to generalize to unseen data. The model has high variance.\n",
    "\n",
    "3. **`Underfitting:`** If both the training loss and the validation loss are high or if they decrease initially and then plateau while still being high, then the model is underfitting. This occurs when the model fails to learn the underlying patterns in the data. The model has high bias.\n",
    "\n",
    "4. **`High Bias and High Variance:`** If the training loss is high and the validation loss is even higher, the model suffers from both high bias and high variance. This means the model is not fitting the training data well and is also not generalizing well to unseen data.\n",
    "\n",
    "### **To rectify these situations:**\n",
    "\n",
    "**`Overfitting:`** Consider using regularization techniques, dropout layers (in case of neural networks), adding more data, reducing model complexity, or early stopping.\n",
    "\n",
    "**`Underfitting:`** Consider making the model more complex. For instance, you might add more layers in a neural network or add more features in a classical machine learning model. You could also consider reducing the amount of regularization if any is being used.\n",
    "\n",
    "**`High Bias and High Variance:`** Diagnosing a model with both high bias and high variance can be tricky as the solutions for high bias and high variance often contradict each other. You may try to find a better balance in the model's complexity, tuning the hyperparameters, adding more useful features, or collecting more data. Model ensembling can also be a good technique to mitigate this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script plots four sets of learning curves for four different scenarios:\n",
    "\n",
    "- `Ideal Case:` Both training and validation losses decrease and eventually plateau.\n",
    "\n",
    "- `Overfitting:` Training loss continues to decrease, but validation loss decreases then starts to increase, indicating the model is overfitting the training data.\n",
    "\n",
    "- `Underfitting:` Both training and validation losses decrease slowly and remain high, indicating the model is not complex enough to learn the data.\n",
    "\n",
    "- `High Bias and Variance: `Training loss is high and validation loss is even higher, indicating the model is not fitting the data well and is not generalizing well either.\n",
    "\n",
    "In reality, you might see variations of these trends and may also see some noise in the plots depending on the specific dataset, model, and training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Other cases:**\n",
    "- **`Early Stopping:`** In this case, the model starts to overfit after a certain number of epochs. Early stopping allows us to prevent overfitting by stopping the training once the validation loss starts to increase..\n",
    "\n",
    "- **`Learning Rate Too High:`** If the learning rate is too high, the loss may not decrease and could even increase explosively.\n",
    "\n",
    "- **`Learning Rate Too Low:`** If the learning rate is too low, the loss decreases very slowly and might never reach its minimum.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots:\n",
    "\n",
    "**`Early Stopping:`** Training loss and validation loss stop decreasing at the same point, indicating that training was stopped early to prevent overfitting.\n",
    "\n",
    "**`High Learning Rate:`** The losses fluctuate wildly, which can be a symptom of a learning rate that's too high.\n",
    "\n",
    "**`Low Learning Rate:`** The losses decrease very slowly, which can indicate a learning rate that's too low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
